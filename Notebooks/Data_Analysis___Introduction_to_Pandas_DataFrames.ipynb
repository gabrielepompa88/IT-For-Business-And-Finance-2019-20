{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - Introduction to Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: [Gabriele Pompa](https://www.linkedin.com/in/gabrielepompa/): gabriele.pompa@unisi.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resources**: \n",
    "\n",
    "- [_Python for Finance (2nd ed.)_](http://shop.oreilly.com/product/0636920117728.do): Sec. 5.The DataFrame Class, 5.Basic Analytics, 5.Basic Visualization, 5.The Series Class, 5.Complex Selection, 5.Concatenation, Joining, and Merging, 5.Performance Aspects.\n",
    "\n",
    "\n",
    "- _[Pandas - Intro to data structures](https://pandas.pydata.org/docs/getting_started/dsintro.html#intro-to-data-structures)_ ([Series](https://pandas.pydata.org/docs/getting_started/dsintro.html#series); [DataFrame](https://pandas.pydata.org/docs/getting_started/dsintro.html#dataframe)). From _[Pandas - Getting started tutorials](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html):_ \n",
    "    - [What kind of data does pandas handle?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html), \n",
    "    - _[How do I select a subset of a `DataFrame`?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html),_ \n",
    "    - _[How to create plots in pandas?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/04_plotting.html),_ _[How to create new columns derived from existing columns](https://pandas.pydata.org/docs/getting_started/intro_tutorials/05_add_columns.html),_ \n",
    "    - _[How to calculate summary statistics?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html),_ \n",
    "    - _[How to combine data from multiple tables?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/08_combine_dataframes.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary <a name=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "# for Pandas Series and DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# for Matplotlib plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DataFrames <a name=\"DataFrame\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Pandas DataFrame](https://pandas.pydata.org/docs/getting_started/dsintro.html#dataframe) is a 2-dimensional labeled data structure with columns of potentially different types (integers, strings, floating point numbers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of it like an Excel  spreadsheet, or - knowing Pandas Series - a dict of Series objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Creation <a name=\"df_creation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be created using the constructor:\n",
    "\n",
    "`\n",
    "pd.DataFrame(data[, index, columns])\n",
    "`\n",
    "\n",
    "where:\n",
    "- `data`: is the data content of the DataFrame. It can be a NumPy N-dim array, a Python dict of:\n",
    "    - Pandas Series,\n",
    "    - 1-dim NumPy arrays, \n",
    "    - Python Lists,\n",
    "    - etc...\n",
    "    \n",
    "- `index`: (optional) is the index of the DataFrame. It can be an array-like structure (e.g. a list). If not provided, default index spans the rows of `data`. For example, if `data` is a NumPy `(n,m)` array, it is `[0,1,...,n-1]`.\n",
    "\n",
    "- `columns`: (optional) lists the columns of the DataFrame. It can be an array-like strucutre (e.g. a list). If not provided, default columns spans the columns of `data`. For example, if `data` is a NumPy `(n,m)` array, it is `[0,1,...,m-1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the creation of a Pandas DataFrame from a 2-dimensional NumPy array (that is, a matrix). We refer to [Intro to data structures - DataFrame](https://pandas.pydata.org/docs/getting_started/dsintro.html#dataframe) for other creational paradigms and full details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's define a $10 \\times 5$ matrix with columns the number from 1 to 10 raised to powers 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mat = np.array([[i**k for i in range(1,11)] for k in range(1,6)]).T\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we pass the matrix `mat` as the `data` parameter of `pd.DataFrame()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mat)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `pd.DataFrame()` returns a Pandas DataFrame object. Each element in the table is linked to its corresponding index and column. \n",
    "\n",
    "Notice that:\n",
    "- the `index` which is generated by default (since we didn't provide one explicitly) is `0, 1,..., 9 = mat.shape[0]-1`;\n",
    "- the `columns` which are generated by default (since we didn't provide them explicitly) are `0, 1,..., 4 = mat.shape[1]-1`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the explicit assignment `data=mat` is optional and equivalent to `pd.DataFrame(mat)`.\n",
    "\n",
    "Similarly to NumPy arrays and Pandas Series, Pandas DataFrames have meta-informative attributes too. Let's have a look at some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of elements is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows and columns of the DataFrame is, similarly to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of the DataFrame may have different data-type, use `.dtypes` attribute to retrieve them (mind the plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a `pd.Series` is returned with each column's data-type reported as a `str` (`object` is the Pandas for `str` data-type) and linked to an index labelled as the corresponding column label in the DataFrame (another example later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to Series, you can directly access the index sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen for Series, `RangeIndex` is the kind of `[0,1,...,mat.shape[0]-1]` index which Pandas creates by default when you don't input one explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, a `[0,1,...,mat.shape[1]-1]` `RangeIndex` is created to label the columns when you don't provide them explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can give more descriptive names to the columns of our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['x', 'x^2', 'x^3', 'x^4', 'x^5']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the columns that we define are a general `Index` of strings (`dtype='object'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Pandas Series, ff you want just the values (without the indexing) - that is, the original NumPy `mat` in our case - these can be accessed through the `.values` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Time indexes: `pd.date_range()` and `pd.to_datetime()`  <a name=\"df_time_index\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exactly the same way we were able to define time-indexes for Pandas Series, we can do it for Pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a range of business days (denoted by the _frequency_ `freq='B'`) starting from Jan 1st 2020. The range lasts a number of `periods` equal to the rows of our DataFrame (`df.shape[0]` which would of course be the same as `mat.shape[0]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2020-01-01', periods=df.shape[0], freq='B') \n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kind of time-index that is returned is a `DatetimeIndex`. We can re-index our DataFrame with this new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = dates\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could have defined the DataFrame with our desired `data`, `index` and `columns` setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=mat, \n",
    "                  index=dates, \n",
    "                  columns=['x', 'x^2', 'x^3', 'x^4', 'x^5'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `pd.to_datetime()` has already been introduced in dedicated [Data Analysis - Introduction to Pandas Series](https://github.com/gabrielepompa88/IT-For-Business-And-Finance-2019-20/blob/master/Notebooks/Data_Analysis___Introduction_to_Pandas_Series.ipynb) notebook as a converter from a list of dates strings to a `DatetimeIndex` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(['2020-01-02', '2020-01-07', '2020-01-10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as for Pandas Series, this function allows to filter rows of a DataFrame according to a list of strings representing dates. We'll see it shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Basic plotting: `df.plot()` and `df.plot.bar()`  <a name=\"df_plot\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Pandas Series, plotting a DataFrame cannot be easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each columns, as reported in the picture legend, is translated into a line of different color, wherheas the common index is used on the x-axis to draw syncronized values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, given that our data are power functions, which grow at different speeds, it's difficult to appreciate all them together in the same plot... if we plot them straight-away. Let's plot their logs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's define a new DataFrame `df_log` which has the same index and columns of the original `df`, but each element get's transformed throug a $\\ln()$ function. \n",
    "\n",
    "We anticipate here the flexibility of NumPy's universal functions, which most can be used with `pandas.DataFrame` parameters in input (instead of `numpy.ndarray`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_En passant,_ we re-label the columns of the `df_log` too. Notice the use of list comprehension together with the `+` operator to concatenate `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = np.log(df)\n",
    "df_log.columns = ['log(' + c + ')' for c in df.columns]\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_log.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything looks much clearer. As we did for Pandas Series, of course we can produce bar-plots (as well as the tons of [visualization styles](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) which are available in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Indexing and Slicing <a name=\"df_selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we describe the different possibilities that you have to access elements of a DataFrame `df`. You can:\n",
    "\n",
    "- Select columns using the `[]` access operator;\n",
    "- Filter rows according to a logical condition using the `[]` access operator;\n",
    "- Select specific rows and columns using column names using `.loc[]` access operator;\n",
    "- Select specific rows and columns using numerical positional indexes in the table using `.iloc[]` access operator.\n",
    "\n",
    "We consider each case in a separate section with examples using our DataFrame of power functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Selecting columns: `[]` <a name=\"df_selection_cols\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we describe how you can select entire columns of a DataFrame. You can use the `[]` access operator. Here is an overview: \n",
    "\n",
    "&nbsp;  | syntax | example\n",
    ":---: | :---: | :---:\n",
    "select 1 column  | `df[colName]` | `s['x^2']`  \n",
    "select  $\\geq 2$ columns  | `df[[listOfCols]]` | `df[['x^2', 'x^3', 'x^5']]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a column using the column name inside the `[]` access operator, as if the DataFrame was a dict of the column-Series with column names its keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x2 = df['x^2']\n",
    "s_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that - having selected just one column - a Pandas Series is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to select more than one column, just wrap their names in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x235 = df[['x^2', 'x^3', 'x^5']]\n",
    "df_x235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and notice that a new DataFrame is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_x235)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Conditional Selection: filtering rows <a name=\"df_conditional_selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we describe how you can filter specific rows according to a logical condition. You can input a logical condition `logicalCondition` to the `[]` access operator, like `df[logicalCondition]`. Here is an overview:\n",
    "\n",
    "examples | meaning\n",
    " :---: |  :---: \n",
    "  `df[df['x^2'] > 5]` | all rows s.t. values in 'x^2' col are > 5\n",
    "  `df[df['x^2'] <= df['x']]` | all rows s.t. values on 'x^2' col are <= than values in 'x' col\n",
    "  `df[df['rating'].isin(['AAA', 'AA', 'A'])]` | all rows s.t. values in 'rating' col are in the list ['AAA', 'AA', 'A'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you want to filter the rows for which column `x^2` is greater than a given threshold, say 17. The logical condition to achieve this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x^2'] > 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that what is returned is a Pandas Series that is named after column `x^2` and that has `True`/`False` boolean values according to whether the condition is satisfied or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this Series of boolean values to actually filter rows of the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['x^2'] > 17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output is the original DataFrame (all columns) with only the rows for that the Series `df['x^2'] > 17` has `True` value. That is to say, only those rows for which the conditon $x^2 > 17$ is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logical condition might involve other rows too. For example, let's define a new DataFrame with power-like columns of decimal number $d \\in [0,2]$ range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_decimal = np.array([[i**k for i in np.linspace(0,2,10)] for k in range(1,6)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in np.linspace(0,2,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_decimal = pd.DataFrame(data = mat_decimal,\n",
    "                          index = x_axis,\n",
    "                          columns = df.columns)\n",
    "\n",
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that to index the DataFrame we use a standard grid of the $[0,2]$ segment on the x-axis, made of 10 linearly spaced points. Of course, it coincides with column `x`, as the latter represents the identity function $f(x)=x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the first two columns `x` and `x^2` and let's see for which values of the x-axis the $x^2 \\leq x$. A plot might helps here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal[['x', 'x^2']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see it? The touching point $x = x^2$ is at $x=1$. For $x<1$, $x^2 < x$. Let's filter rows according to this condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal['x^2'] <= df_decimal['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, we can use this Series of booleans to filter rows of `df_decimal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal[df_decimal['x^2'] <= df_decimal['x']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, as expected, returns only rows for which the `x_axis` index is at most 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may want to filter rows according to values in a list. Suppose we have a list of Standard & Poor's _high grade_ [credit ratings](https://en.wikipedia.org/wiki/Credit_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highGradeRatings = ['AA+', 'AA', 'AA-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a DataFrame of some reference data (country, S&P credit ratings and corresponding spreads in [bps](https://en.wikipedia.org/wiki/Basis_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData = pd.DataFrame(data={\n",
    "                             'S&P Rating': ['A', 'BB', 'AA', 'CCC'],\n",
    "                             'Spread': [100, 300, 70, 700],\n",
    "                             'Country': ['USA', 'ITA', 'UK', 'ITA']\n",
    "                            },\n",
    "                       index=['Firm_1', 'Firm_2', 'Firm_3', 'Firm_4'])\n",
    "\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have used a `dict` of Python `list` as `data` parameter in `pd.DataFrame` constructor, which provides both values and column names (which allows to skip `columns` parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now suppose we want only want the rows (the firms) having a credit rating which is among the `highGradeRatings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this row-filtering using the `.isin()` method, which tests whether values of a column are in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData['S&P Rating'].isin(highGradeRatings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and returns a Pandas Series of bools. You can use this boolean Series to filter rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData[df_refData['S&P Rating'].isin(highGradeRatings)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, as expected, returns only row for `Firm_3`, that is the only firm with a rating in the `highGradeRatings` list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Selecting rows with rows and columns _names_ : `.loc[]` <a name=\"df_loc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we describe how you can select specific rows and columns using their labels. You can use the `.loc[ row indexer, column indexer ]` access operator using labels to identify rows and columns. For both the `row indexer`, before the comma, and the `column indexer`, after the comma, you can have several options. We review them in the next sub-sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can combine different kind of indexers (single, list, slice, conditonal) for row and col indexers separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.1. Single label <a name=\"df_loc_single\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single label can be used to get one row and/or column only:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "label | label | `df.loc[indexLabel, colName]` | element in col `colName` at row `indexLabel`\n",
    "      |       | `df.loc['2020-01-02', 'x^2']` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the single element in column `x^2` and at row of index '2020-01-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-06', 'x^2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, accessing using an index and/or a column that doesn't exist, raises a `KeyError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises KeyError as '2020-02-01' is not an index of df\n",
    "#\n",
    "# df.loc['2020-02-01', 'x^2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises KeyError as 'x^7' is not a column of df\n",
    "#\n",
    "# df.loc['2020-01-06', 'x^7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.2. List of labels <a name=\"df_loc_list\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of labels can be used to get a list of rows and/or columns, using `pd.to_datetime()` in case of (typically) indexes which are Dates Strings:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "... | names list | `df.loc[..., [listOfCols]]` | elements in `[listOfCols]` cols at rows...\n",
    "      |            | `df.loc[..., ['x^2', 'x^3', 'x^5']]`  |   \n",
    "labels list  | ... | `df.loc[[listOfIndexLabels], ...]` | elements at `[listOfIndexLabels]` rows and in cols...\n",
    "             |       | `df.loc[['a', 'c', 'd'], ...]` |   \n",
    "dates list   | ... | `df.loc[pd.to_datetime([listOfDatesStrings]), ...]` | elements at `[listOfDatesStrings]` rows and in cols...\n",
    "             |       | `df.loc[pd.to_datetime(['2020-01-02', '2020-01-05']), ...]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a list of columns and rows to use for the selection. Let's use `df_refData` for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenceData = ['S&P Rating', 'Spread']\n",
    "firms = ['Firm_1', 'Firm_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and select rows and/or columns accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.loc[firms, referenceData]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to `df`. DataFrame `df` has Dates as index (a `DatetimeIndex`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as already noticed for Pandas Series, when you want to filter rows according to a list of strings representing dates, you have to convert them into a valid `DatetimeIndex` object using `pd.to_datetime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's choose a few dates and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "someDates = ['2020-01-03', '2020-01-06', '2020-01-09']\n",
    "oddsPowerCols = ['x', 'x^3', 'x^5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[pd.to_datetime(someDates), oddsPowerCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected, the desired selection of odds powers at the requested dates is returned.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already noticed for Pandas Series ( _repetita iuvant_ ) using the dates strings straight away as row indexer would raise a `KeyError` because Pandas tries to look for those raw stirngs among the indexes, whereas what you actually meant were dates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises KeyError\n",
    "#\n",
    "# df.loc[someDates, oddsPowerCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.3. Slice of labels <a name=\"df_loc_slice\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice of labels can be used to get a slice of rows and/or columns:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "slice | ... | `df.loc[fromIndexLabel:toIndexLabel:eachTotRows, ...]` | elements at rows as per slice and in cols...\n",
    "      |       | `df.loc['2020-01-06':'2020-01-09', ...]` |\n",
    "...   | slice | `df.loc[..., fromColName:toColName:eachTotCols]` | elements in cols as per slice and at rows...\n",
    "      |       | `df.loc[..., 'x':'x^3']` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we may be interested to the last few dates values of odds powers columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09':, oddsPowerCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the slice `'2020-01-09':` selects all the dates indexes from '2020-01-09' to the end, whereas we use a list of column names as cols indexer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may instead be interested in a slice of columns. This is possible too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09', 'x':'x^3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a Series is returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use simultaneously slices both for row indexers and col indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09':, 'x':'x^3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that, as expected returns a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.4. Conditional Expression <a name=\"df_loc_conditional\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logical conditional expression can be used to filter rows and/or columns (typically according to their names):\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "logical condition  | ... | `df.loc[logicalCondition, ...]` | \n",
    "                   |     | `df.loc[df['x^2'] > 5, ...]` | elements at rows filtered as per logical condition and in cols...\n",
    "...  | logical condition | `df.loc[..., df.columns.isin(listOfNames)]` | elements in cols in the `listOfNames` and at rows...\n",
    "                   |     | `df.loc[..., df.columns.isin(['Pippo', 'x^3', 'Pluto'])]` | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `df_decimal` to make examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's go back to our $x^2 \\leq x$ logical condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal['x^2'] <= df_decimal['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which you can use as a row indexer. If you are interested in all the columns, then fine, we have already seen that you can just use the `[]` access operator with the logical condition only (row filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal[df_decimal['x^2'] <= df_decimal['x']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, if you are not interested in all the columns, but say `x` and `x^2`, you may wrap them in a list to get only them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.loc[df_decimal['x^2'] <= df_decimal['x'], ['x', 'x^2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and notice that we need the `.loc[]` access operator since we are using column indexers too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you have a list o some names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfNames = ['x', 'x^x', 'Pluto', 'x^3', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we see, some of them are columns of `df`, but there are others which are otherwise unrelated names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you can use the `.isin()` method of the `df.columns` attribute, which returns the subset of `df` columns which are in `listOfNames`, and can be used as a column indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09':, df.columns.isin(listOfNames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, as expected, returns all the element at rows from date '2020-01-09' (slice of labels) but only for the `x` and `x^3` columns, which are the only two in the `listOfNames`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, if no column of `df.columns` is in `listOfNames`, a DataFrame without any column is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelatedListOfNames = [ 'Donald Duck', 'Mickey Mouse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09':, df.columns.isin(unrelatedListOfNames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quite a strange object, I agree. Just remember this is caused by the fact that no column of `df` was in `unrelatedLisOfNames`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.5. Colon `:` <a name=\"df_loc_colon\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A colon `:` can be used to specify that you want to select all rows or columns:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "`:`         | `:`         | `df.loc[:, :]` | all the elements (equivalent to just `df`)\n",
    "`:`         | ...         | `df.loc[:, ...]` | elements from all the rows and in cols...\n",
    "...         | `:`         | `df.loc[..., :]` | elements from all the cols and at rows...\n",
    "...         | omitted     | `df.loc[...]` | equivalent to `df.loc[..., :]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want the entire DataFrame you are used to just type `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and keep doing it. Just rememember that writing `df` is actually interpreted as a selection of all rows and columns of the DataFrame, omitting rows and cols indexers. An equivalent way to select all rows and cols, without omitting the indexers is `df.loc[:,:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want all rows but just some columns you can use the colon `:` as a row indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['x', 'x^2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, if you want all the columns, but just a few rows, you can use the colon `:` as a cols indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09':, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if you want all the columns, you can completely omit the column indexer. Therefore, `df.loc['2020-01-09':, :]` is equivalent to `df.loc['2020-01-09':]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2020-01-09':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally notice, that the omission of indexers doens't work symmetrically. You cannot omit the row indexer. \n",
    "\n",
    "That is to say, if you want all the rows but just the `['x', 'x^2']` cols, you can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['x', 'x^2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but you cannot do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises KeyError\n",
    "#\n",
    "# df.loc[['x', 'x^2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `.loc[]` access specifier omitting an indexer, the missing indexer is interpreted as a columns indexer, that's why something like\n",
    "\n",
    "```python\n",
    "df.loc[['x', 'x^2']]\n",
    "```\n",
    "\n",
    "is interpreted as \n",
    "\n",
    "```python\n",
    "df.loc[['x', 'x^2'], :]\n",
    "```\n",
    "\n",
    "which makes Pandas looking for rows with names `x` and `x^2` and thus raising a `KeyError` not finding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, you already know how to select all the rows and few cols: you have to use the `[]` access operator straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['x', 'x^2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is equivalent to `df.loc[:, ['x', 'x^2']]`. We can conclude here that there is equivalence between \n",
    "\n",
    "```python\n",
    "df[column_indexer]\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "df.loc[:, column_indexer]\n",
    "```\n",
    "\n",
    "as both return all the rows and the selected columns. Good to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Selecting rows with rows and columns _positional indexes_ : `.iloc[]` <a name=\"df_iloc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we describe how you can select specific rows and columns using their position in the table. You can use the `.iloc[ row positional indexer, column positional indexer ]` access operator using numeric positional indexes to identify rows and columns.\n",
    "\n",
    "Access operator `.iloc[]` is somehow more limited than `.loc[]`. Still you have several options  for both the `row positional indexer`, before the comma, and the `column positional indexer`, after the comma. We review them in the next sub-sections which parallel the different cases examined for `.loc[]`.\n",
    "\n",
    "Of course you can combine different kind of indexers (single, list, slice, conditonal) for row and col indexers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can combine different kind of indexers (single, list, slice, conditonal) for row and col indexers separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access operator `.iloc[]` is somehow more limited than `.loc[]`. Still you have several options  for both the `row positional indexer`, before the comma, and the `column positional indexer`, after the comma.\n",
    "\n",
    "In particular, both row and column indexer can be:\n",
    "\n",
    "- A positional index:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    " position  | position | `df.iloc[i,m]` | element in col position `m` at row position `i`\n",
    "           |          | `df.iloc[1, 3]`| \n",
    "\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "position | position | `df.loc[indexLabel, colName]` | element in col `colName` at row `indexLabel`\n",
    "      |       | `df.loc['2020-01-02', 'x^2']` |\n",
    "\n",
    "\n",
    "row indexer  | col indexer | syntax | example \n",
    ":---: | :---: | :---: | :---:\n",
    " position  | position | `df.iloc[i,m]` | `df.iloc[1, 3]`  \n",
    "slice  | slice | `df.iloc[i:j:k, m:n:q]` | `df.iloc[1:3:2, 2:3]` \n",
    "list of positions  | list of positions | `df.iloc[[i,l],[m,p]]` | `df.iloc[[1,2,3], [6,5,4]]`\n",
    "\n",
    "we consider each case in separated sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.1. Single positional index <a name=\"df_iloc_single\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single positional index label can be used to get one row and/or column only:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    " position  | position | `df.iloc[i,m]` | element in col position `m` at row position `i`\n",
    "           |          | `df.iloc[1, 3]`| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the single element in column 4th column and at 2nd row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing using an indexes out of bounds, raises a `IndexError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises IndexError as 100 is out of index positional bounds\n",
    "#\n",
    "# df.iloc[100,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises IndexError as 10 is out of columns positional bounds\n",
    "#\n",
    "# df.iloc[1,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.2. List of positional indexes <a name=\"df_iloc_list\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of positional indexes can be used to get a list of rows and/or columns√ß\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "... | positions list | `df.iloc[..., [listOfColsPos]]` | elements in cols positions `[listOfColsPos]` at rows...\n",
    "      |            | `df.iloc[..., [1,2,4]]`  |   \n",
    "positions list  | ... | `df.iloc[[listOfRowsPos], ...]` | elements at rows positions `[listOfRowsPos]` in cols...\n",
    "             |       | `df.loc[[0,3,5], ...]` |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting lists of rows and/or cols according to their positions is straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[0,3,5], [1,2,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and works as expected. Other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, [1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[0,3,5], 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.3. Slice of positional indexes <a name=\"df_iloc_slice\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice of positional indexes can be used to get a slice of rows and/or columns:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "slice | ... | `df.iloc[i:j:k, ...]` | elements at rows as per slice `i:j:k` and in cols...\n",
    "      |       | `df.iloc[1:7:2, ...]` |\n",
    "...   | slice | `df.iloc[..., m:n:q]` | elements in cols as per slice `m:n:q` and at rows...\n",
    "      |       | `df.iloc[..., 1:3]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can slice rows and/or columns. A few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1:7:2, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the slice `1:7:2` selects all the rows from position 1 (included) to 7 (excluded), each 2 rows and the slice `1:3` selects each column from position 1 (included) to position 3 (excluded). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine with other ways of positional indexing. Other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[2,3,4], 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.4. Conditional Expression to filter rows <a name=\"df_iloc_conditional\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logical conditional expression can be used to filter rows but it cannot be a NumPy array of booleans, not a Pandas Series:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "logical condition (array) | ... | `df.iloc[(logicalCondition).values, ...]` | \n",
    "                   |     | `df.iloc[df['x^2'].values > 5, ...]` | elements at rows filtered as per logical condition and in cols...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of `.iloc[]` with conditional expression is somehow nastier and I rarely use it. But still.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our $x^2 \\leq x$ logical condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal['x^2'] <= df_decimal['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to input that condition as a row indexer in the `.iloc[]` - as you do with the `.loc[]` access operator - and input a positional index as a column indexer. But this raises a `ValueError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raises ValueError\n",
    "#\n",
    "# df_decimal.iloc[df_decimal['x^2'] <= df_decimal['x'], [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact is that the logical condition in the row indexer has to be an array of booleans... how can we transform it? `.values` on the rescue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_decimal['x^2'] <= df_decimal['x']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.iloc[(df_decimal['x^2'] <= df_decimal['x']).values, [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we wanted! The `.values` attribute, returns the values of the boolean Series `df_decimal['x^2'] <= df_decimal['x']`, which is then a NumPy array of booleans. Which we can use a rows indexer in the `.iloc[]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_En passant,_ notice that instead of `.values` the whole expression, you could have alternativelly use `.values` on each Pandas Series involved in the logical expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.iloc[df_decimal['x^2'].values <= df_decimal['x'].values, [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.5. Colon `:` <a name=\"df_iloc_colon\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A colon `:` can be used to specify that you want to select all rows or columns:\n",
    "\n",
    "row indexer | col indexer | syntax/example| meaning\n",
    ":---:       | :---:       | :---:  | :---: \n",
    "`:`         | `:`         | `df.iloc[:, :]` | all the elements (equivalent to just `df`)\n",
    "`:`         | ...         | `df.iloc[:, ...]` | elements from all the rows and in cols...\n",
    "...         | `:`         | `df.iloc[..., :]` | elements from all the cols and at rows...\n",
    "...         | omitted     | `df.iloc[...]` | equivalent to `df.iloc[..., :]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice this is exactly the same behavior that you have with `.loc[]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, `df.iloc[:,:]` is equivalent to just typing `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want all rows but just some columns you can use the colon `:` as a row indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, if you want all the columns, but just a few rows, you can use the colon `:` as a cols indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[6:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if you want all the columns, you can completely omit the column indexer. Therefore, `df.iloc[6:, :]` is equivalent to `df.iloc[6:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, notice that - as already pointed out in the `.loc[]` case - if you omit the row indexer to ask for all the rows and few cols, you get the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is, `[0,2]` gets interpreted as row positions. That's because if a single indexer is provided to `.iloc[]`, it gets interpreted as row indexer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the `[0,2]` columns you have to use the colon `:` as row indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, [0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Creating (and deleting) New Columns <a name=\"df_new_cols\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new columns is as simple as defining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to share price (in USD) for each firm as an additional column `Share Price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData['Share Price'] = [21.5, 15.0, 32.25, 2.5]\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, isn't it? New values, wrapped in the list, are assigned according to their input order, to the corresponding index of `df_refData`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of outstanding shares too, as new column `Number of Shares` (in billions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData['Number of Shares'] = [20, 3, 5, 2]\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compute the market capitalization `Market Cap` for each firm, as \n",
    "\n",
    "`Market Cap` $=$ `Share Price` $\\times$ `Number of Shares`, \n",
    "\n",
    "which serves here as an example of a column defined from two other existing columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData['Market Cap'] = df_refData['Share Price'] * df_refData['Number of Shares']\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see, each product of the share price and the number of shares is computed _elementwise_. This is a recurring feature, wwe have already observed several times (e.g. NumPy arrays methods, Pandas Series methods, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we decide we no longer want to keep only the `Market Cap`? You can delete columns as you would to with `key: value` pairs of a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_refData['Share Price']\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_refData['Number of Shares']\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you can define new columns also according to logical conditions applied to other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData['isBlueChip'] = df_refData['Market Cap'] > 100.0\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, the logical condition that defines the new column `isBlueChip` works elementwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, notice that the each column might have in principle, its own data-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `int64` and `float64` are for integers and floating values, respectively, `object` is for strings and `bool` is for boolean values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Basic Analytics <a name=\"df_analytics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As direct generalization of Pandas Series, Pandas DataFrames too feature vectorized operations, a lot of built-in methods and can be safely passed to most of NumPy universal functions (that would expect NumPy arrays in input). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. _Vectorized_ operations <a name=\"df_basic_operations\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame-Number and DataFrame-DataFrame operations are vectorized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal + 10*df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Built-in methods <a name=\"df_builtin_methods\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are tons of built-in methods. By default, built-in methods work column-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a Series of the sums of each column of `df_decimal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the `axis` parameter to have the corresponding row-wise result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which returns a Series of the sums of each row of `df_decimal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3. Interoperability with NumPy's universal functions <a name=\"df_ufunc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of NumPy universal functions, which expect NumPy arrays in input, work with Pandas DataFrames in input as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(df_decimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4. `.groupby()` category <a name=\"df_groupby\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics can be performed on a per-group base, using the `.groupby()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide our DataFrame in two groups according to whether $x <= 1$ or $x > 1$. This can be easily achieved defining a column `x range` of strings `x > 1` or `x <= 1` according to whether columns `'x'` have values greater or smaller-or-equal than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal['x range'] = ['x > 1' if x > 1 else 'x <= 1' for x in df_decimal['x']]\n",
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going forward, just notice how we used list comprehension to define the new `x range` column of strings. What does \n",
    "\n",
    "```python\n",
    "['x > 1' if x > 1 else 'x <= 1' for x in df_decimal['x']]\n",
    "```\n",
    "do? (try to answer yourself before reading below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy: it is a list, as is surrounded by square brackets `[]`. Ok, a list of what? Well, the expression\n",
    "\n",
    "```python\n",
    "for x in df_decimal['x']\n",
    "```\n",
    "defines a loop over the values in column `x` of `df_decimal`, which at each loop iteration gives the dummy name `x` to value considered at that iteration. Ok, so? Well, each value `x` is then checked whether it is `x > 1` or not. Good... so how is the list filled then? Well, the list is filled with string `'x > 1'` at iterations in which the dummy variable satisfies condition `x > 1` and with string `'x <= 1'` otherwise. Bravo! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this grouping column `x range`, we can compute `.sum()`, `.mean()`, `.std()` and whatever you want on the two seperate groups just prepending the grouping condition `.groupby('x range')` to the method that you want to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.groupby(\"x range\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this returns the `.sum()` over columns, separately for each group of rows, according to the grouping defined by `x range`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we were interested in the group `.mean()` of `'x^5'` column only. Well easy, just use the `[]` access operator after the grouping statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.groupby('x range')['x^5'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can count the number of rows in each group: `.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.groupby('x range').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can group by more than group simultaneously... Let's define another group, called `x over x^2` which marks the rows according to whether $x > x^2$, $x = x^2$ or $x < x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal['x over x^2'] = ['x > x^2' if x > x2 else 'x = x^2' if x == x2 else 'x < x^2' \n",
    "                            for (x,x2) in zip(df_decimal['x'], df_decimal['x^2'])]\n",
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the hell did you define the `x over x^2` column? Well, it's a bit harder, but not that much... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to understand is a the built-in [function `zip()`](https://docs.python.org/3.3/library/functions.html#zip).\n",
    "\n",
    "It works as follows: you have to use `zip()` in a loop when you want to iterate over elements of several sequences at the same time. An example: say we want to prints the pairs of elements coming from two lists of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = [6,7,8,9,10]\n",
    "\n",
    "for (ai, bi) in zip(a,b):\n",
    "    print(ai, bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you can see, `zip()` in a loop iterate over the pairs of elements (a Tuple, whose elements we called `(ai, bi)`), one from each list. It's not limited to just two lists of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = [6,7,8,9,10]\n",
    "c = [11, 12, 13, 14, 15]\n",
    "\n",
    "for (ai, bi, ci) in zip(a,b,c):\n",
    "    print(ai, bi, ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now what \n",
    "\n",
    "```python\n",
    "for (x,x2) in zip(df_decimal['x'], df_decimal['x^2'])\n",
    "```\n",
    "\n",
    "is, it's a bit more clear: it defines a loop over the values in the pair of columns `x` and `x^2` a of `df_decimal` and, at each loop iteration, it gives the dummy names `x` and `x2` to the elements of the Tuple returned by `zip()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the rest of the expression\n",
    "\n",
    "```python\n",
    "'x > x^2' if x > x2 else 'x = x^2' if x == x2 else 'x < x^2' \n",
    "```\n",
    "\n",
    "is simply the specification of the values that we want to have in column `x over x^2`: depending on the values of the dummy variables `x` and `x2`, the column is filled with `'x > x^2'`, `'x = x^2'` or `'x < x^2'`. A pseudo-code for this is:\n",
    "\n",
    "```python\n",
    "if x > x2:\n",
    "    # fill with value: 'x > x^2'\n",
    "    else:\n",
    "        if x == x2:\n",
    "            # fill with value 'x = x^2'\n",
    "        else:\n",
    "            # fill with value 'x < x^2'\n",
    "```\n",
    "\n",
    "Ok, eonough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have two columns, `x range` and `x over x^2`, that represent possible groupings for our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an analytics, like `.sum()` over compound groups can be done in the same way as before, just wrap the grouping columns as a list in the `.groupby()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.groupby(['x range', 'x over x^2']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that shows column-wise sums divided by the double criterion $x > 1$/$x \\leq 1$ and $x > x^2$, $x = x^2$ or $x < x^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as before you can count the rows in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.groupby(['x range', 'x over x^2']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can filter just few columns if you want, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decimal.groupby(['x range', 'x over x^2'])['x^5'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Data Alignment <a name=\"df_data_alignment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talking about Pandas Series, we have observed that two different Series, when combined, gets aligned accoridng to their indexes. As direct extension, Pandas DataFrames consider also columns to align data when combining to DataFrames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:'2020-01-08', ['x', 'x^2', 'x^3']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc['2020-01-04':, ['x^3', 'x^4', 'x^5']]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, `df1` and `df2` share:\n",
    "- indexes from `2020-01-06` to `2020-01-08`\n",
    "- column `x^3`\n",
    "\n",
    "how, then, an operation as simple as `df1 + df2` is defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_1plus2 = df1 + df2\n",
    "df_1plus2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noticed for indexes only in Pandas Series, combined DataFrame will have the _union_ of both indexes and columns, with operations performed elementwise and only where the `[row, column]` is shared by both DataFrames (here `[2020-01-06, 'x^3]`, `[2020-01-07, 'x^3]` and `[2020-01-08, 'x^3]`) and putting a `NaN` elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this generalizes to any other operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1times2 = df1 * df2\n",
    "df_1times2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is the case for Pandas Series, most basic analytics still works disregarding NaNs. That is, `NaN` are not counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2.sum(axis=0) #columns-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2.sum(axis=1) #row-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2.mean(axis=0) # column-wise mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2.mean(axis=1) # row-wise mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2.std(axis=0) # column-wise standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1plus2.std(axis=1) # row-wise standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Combine data from multiple DataFrames <a name=\"df_data_combine\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we explore two ways to combine two DataFrames: concatenating or joining them. Let's work with our `df` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which features 10 rows and 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1. Concatenating DataFrames: `pd.concat()` <a name=\"df_data_concat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function \n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2,...], axis=0, sort=True)\n",
    "```\n",
    "\n",
    "concatenates the list of (two or more) DataFrames according to the `axis`:\n",
    "\n",
    "- 0, default, for rows-wise concatenation (that is, vertically), or\n",
    "- 1, for column-wise concatenation (that is, horizontally)\n",
    "\n",
    "and sorts (if `sort=True`, default) or leaves untouched (if `sort=False`) the non-concatenating axis, that is\n",
    "\n",
    "- columns in case of vertical concatenation, or\n",
    "- rows, in case of horizontal concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1.1. Vertical concatenation: `pd.concat(...[, axis=0])` <a name=\"df_data_concat_0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start considering _vertical_ concatenation. We define two DataFrames, considering two - non overlapping - slices of rows of the `df` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up = df.loc[:'2020-01-08']\n",
    "df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_down = df.loc['2020-01-09':]\n",
    "df_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_down.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `df_up` and `df_down` share the same columns and do not have overlapping indexes. Let's concatenate them one over the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_up_down = pd.concat([df_up, df_down])\n",
    "df_up_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_down.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `df_up_down` is simply given by the vertical stacking of `df_up` and `df_down`, with columns of the same name matched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now keep the same columns, but introducing an overlap in the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_overlap = df.loc[:'2020-01-08']\n",
    "df_up_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_overlap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_down_overlap = df.loc['2020-01-04':]\n",
    "df_down_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_down_overlap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_down_overlap = pd.concat([df_up_overlap, df_down_overlap])\n",
    "df_up_down_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_down_overlap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that indexes are simply stacked one over the other, such that the resulting `df_up_down_overlap` DataFrame has repeated indexes: '2020-01-06', '2020-01-07' and '2020-01-08'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1.2. Horizontal concatenation: `pd.concat(..., axis=1)` <a name=\"df_data_concat_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider _horizontal_ concatenation. We consider `df_refData` DataFrame and another DataFrame `df_otherRefData` with additional reference data for some firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData = pd.DataFrame(\n",
    "                    data={\n",
    "                        \"Moody's Rating\": ['A2', 'Ba2', 'Aa2', 'Caa2'], # notice use of \"\" to allow apostrophe ' in string\n",
    "                        'Fitch Rating': ['A', 'BB', 'AA', 'CCC']\n",
    "                    },\n",
    "                    index=['Firm_1', 'Firm_2', 'Firm_3', 'Firm_4'])\n",
    "\n",
    "df_otherRefData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `df_otherRefData` and `df_refData` share the same rows and do not have overlapping columns. Let's concatenate them one next to the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_completeRefData = pd.concat([df_refData, df_otherRefData], axis=1)\n",
    "df_completeRefData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completeRefData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `df_completeRefData` is simply given by the horizontal stacking of `df_refData` and `df_otherRefData`, with common indexes matched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now keep the same indexes, but introducing an overlap in the columns, such that `Spread` column is shared by both `df_refData` and `df_otherRefData_overlap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData_overlap = pd.DataFrame(\n",
    "                             data={\n",
    "                                   \"Moody's Rating\": ['A2', 'Ba2', 'Aa2', 'Caa2'],\n",
    "                                   'Fitch Rating': ['A', 'BB', 'AA', 'CCC'],\n",
    "                                   'Spread': [100, 300, 70, 700]\n",
    "                                  },\n",
    "                             index=['Firm_1', 'Firm_2', 'Firm_3', 'Firm_4'])\n",
    "\n",
    "df_otherRefData_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData_overlap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completeRefData_overlap = pd.concat([df_refData, df_otherRefData_overlap], axis=1)\n",
    "df_completeRefData_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData_overlap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that columns are simply stacked one next to the other, such that the resulting `df_otherRefData_overlap` DataFrame has repeated column: `'Spread'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only scratched the surface of the concatenation possibilities offered by Pandas, a good further reading can be found in the [user guide: Concatenating objects](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take-home message**: we typically use concatenation when we have two DataFrames that are clearly complementary. For example, \n",
    "\n",
    "- you might need to _vertically_ concatenate datasets of the same kind of data that you have downloaded sequentially and that thus have consecutive and non-overallaping Dates indexes, like `df_up` and `df_down` in our example. In this case it may be useful to vertically stack the two, to have a longer historical time series of data.\n",
    "\n",
    "\n",
    "- you might need to _horizontally_ concatenate two reference data datasets, both containing reference data for the same set of entities, but one containing some reference data, like `'S&P Rating'`, and the other one, complementary reference data, like `\"Moody's Rating\"` and `'Fitch Rating'`. And thus the two sets of columns are non-overlapping. In this case it may be useful to horizontally stack the two, to have a more complete dataset for each instrument.\n",
    "\n",
    "If you have overlaps in indexes and/or columns, you probably should be looking for a `.join()` of the two DataFrames..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2. Joining DataFrames: `.join()` and `pd.merge()` <a name=\"df_data_join_and_merge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a lot of built-in functionalities to do in-memory join operations between datasets, in most aspects similiar to relational databases like SQL. [Function `pd.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html#pandas.merge) is the the most general entry point for all standard database join operations between DataFrames (or Series). The related [`.join()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html#pandas.DataFrame.join) (which uses `pd.merge()` internally) of a DataFrame object is, instead, more specifically oriented toward index-on-index and column(s)-on-index join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we analyze a couple of common join situations you may encounter in your daily activities and how to tackle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2.1. Index-on-index join: `df1.join(df2[,how])` <a name=\"df_data_join_ind\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.join()` is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame. A good further reading can be found in the the [user guide: Joining on index](https://pandas.pydata.org/docs/user_guide/merging.html#joining-on-index). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have two DataFrames, say `df1` and `df2`, you can combine them into a single DataFrame using [method `join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html#pandas.DataFrame.join) applied to `df1` and providing `df2` in input. Basic syntax is \n",
    "\n",
    "```python\n",
    "df1.join(df2, how='left')\n",
    "```\n",
    "\n",
    "which joins `df1` with `df2` on the base of their indexes. Parameter `how` can take the following values:\n",
    "\n",
    "- `'left'`, default, which uses `df1` indexes to do a _left-join_ on `df2` indexes, or \n",
    "- `'right'`, which uses `df2` indexes to do a left-join on `df1` indexes (equivalent to `df2.join(df1, how='left')`), or \n",
    "- `'inner'`, which uses indexes in common (the intersection of) between `df1` and `df2` to do an _inner-join_ of `df1` and `df2` indexes, or \n",
    "- `'outer'`, which uses both indexes of (the union of) `df1` and `df2` to do an _outer-join_ of `df1` and `df2` indexes.\n",
    "\n",
    "there is another relevant parameter, `on`, which can be omitted in an index-on-index situation and that we will introduce in the next section when talking about index-on-column join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider our DataFrame `df_refData` which stores some reference data information for `Firm_1`, `Firm_2`, `Firm_3` and `Firm_4`, which are used as indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define another DataFrame, `df_otherRefData`, can be used to complement `df_refData` informations on Moody's and Fitch ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData = pd.DataFrame(\n",
    "                    data={\n",
    "                        \"Moody's Rating\": ['Aa2', 'Caa2', 'A2', 'Aaa', 'Aa3'], # \"\" allows use of apostrophe ' in string\n",
    "                        'Fitch Rating': ['AA', 'CCC', 'A', 'AAA', 'AA-']\n",
    "                    },\n",
    "                    index=['Firm_3', 'Firm_4', 'Firm_1', 'Firm_5', 'Firm_6'])\n",
    "\n",
    "df_otherRefData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `df_otherRefData` has informations on: \n",
    "- `Firm_1`, `Firm_3`, `Firm_4`, indexes shared with `df_refData`;\n",
    "- new firms `Firm_5` and `Firm_6`;\n",
    "- but doesn't have data for `Firm_2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a [left-join](https://en.wikipedia.org/wiki/Join_(SQL)#Left_outer_join) of `df_refData` indexes on `df_otherRefData`. In other words, we want to keep all the rows of `df_refData` and complete them with those included in `df_otherRefData`, when available, on the bases of their shared indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.join(df_otherRefData, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the DataFrame returned has all - and only - the indexes (and rows) of `df_refData`, with data completed with `'Moody's' Rating` and `'Fitch Rating'` additional columns, coming from `df_otherRefData`. Data for `Firm_2`, which are missing in `df_otherRefData`, are marked as `NaN` in the output DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice also that `how` parameter is `how='left'` by default and then can be omitted in the case of a left-join operation as this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.join(df_otherRefData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [right-join](https://en.wikipedia.org/wiki/Join_(SQL)#Right_outer_join) is the symmetric case of the left-join, where rows from `df_otherRefData` are kept, with data completed with those from `df_refData`. This can be achieved either using `how='right'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.join(df_otherRefData, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, as expected, inverting the role of the two DataFrames and doing a left-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otherRefData.join(df_refData, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output includes all the rows from `df_otherRefData`, with data completed with those from `df_refData`. Moreover, data for `Firm_5` and `Firm_6`, which are missing in `df_refData`, are marked as `NaN` in the output DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider an [inner-join](https://en.wikipedia.org/wiki/Join_(SQL)#Inner_join) between `df_refData` and `df_otherRefData` indexes. In other words, we want to keep only the rows in common (on the bases of their shared indexes) between `df_refData` and `df_otherRefData`, completing columns from `df_refData` with those from `df_otherRefData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.join(df_otherRefData, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the DataFrame returned has only the indexes which `df_refData` and `df_otherRefData` have in common. Data from the two DataFrames are integrated adding `'Moody's' Rating` and `'Fitch Rating'` columns, coming from `df_otherRefData`, to those from `df_refData`. Row for `Firm_2`, which is part of `df_refData` but is missing in `df_otherRefData`, is excluded from the output. In the same way, data for `Firm_5` and `Firm_6`, which are part of `df_refData` but missing in `df_otherRefData`, are excluded from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a [outer-join](https://en.wikipedia.org/wiki/Join_(SQL)#Left_outer_join) of `df_refData` indexes on `df_otherRefData`. In other words, we want to keep all the rows of `df_refData` and complete data with those included in `df_otherRefData`, when available, on the base of their shared indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.join(df_otherRefData, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output DataFrames has all the rows from the two DataFrames (you see both `Firm_2` and `Firm_5` and `Firm_6`) and data, missing in one of the other original DataFrame, are marked with `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2.2. Join on Column(s): `df1.join(df2,on)` and `pd.merge(df1, df2, on)` <a name=\"df_data_join_cols\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are situations in which you want to join two DataFrames on the base of a key-column of one DataFrame and the index or another key-column of the other DataFrame.  A good further reading can be found in the the [user guide: Joining on index](https://pandas.pydata.org/docs/user_guide/merging.html#joining-key-columns-on-an-index). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case you have to use the `on` parameter of `.join()` method \n",
    "\n",
    "```python\n",
    "df1.join(df2, how='left', on=None)\n",
    "```\n",
    "\n",
    "You have to give to parameter `on`, which is left unspecified by default (`None` is the Python key-word to say that something is un-specified), the name of a column which is shared by `df1` and `df2` and on the base of which you want to perform the join. The `how` parameter keeps the same meaning and the idea is that you can do a left/righ/inner/outer-join on the base of the column shared and specified by `on`, instead of the indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete `df_refData` with a `'Ticker'` column, which might represent the Bloomberg (or Reuters, or other data source) identifier for each firm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData['Ticker'] = ['CDE', 'BCD', 'DEF', 'ABC']\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have another table `df_dividendInfo` which store some dividend informations. These informations are organized according to a list of ticker. Think of it as a big database where dividend informations for all the companies (not only those in your portfolio `df_refData`) are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dividendInfo = pd.DataFrame(\n",
    "                    data={\n",
    "                        'Dividend Yield (%)': [0.72, 0.45, 1.15, 0.96, 2.01, 1.5, 0.3, 0.89],\n",
    "                        'Dividend Frequency': ['Quarterly', \n",
    "                                               'Monthly', \n",
    "                                               'Biannually',\n",
    "                                               'Quarterly',\n",
    "                                               'Biannually',\n",
    "                                               'Annually',\n",
    "                                               'Monthly',\n",
    "                                               'Quarterly']\n",
    "                    },\n",
    "                    index=['ABC', 'BCD', 'CDE', 'DEF', 'EFG', 'FGH', 'GHI', 'HIJ'])\n",
    "\n",
    "df_dividendInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to complete our `df_refData` dataset with dividend informations. To do this, we can left-join `df_refData` `on` the `'Ticker'` key column, with the indexes of `df_dividendInfo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, in a case like this, where you have a dataset `df_refData` with reference data for the firms in your portfolio and a data-source `df_dividendInfo` with data for a whole set of firms, the only meaningful join is the left-join of `df_refData` and `df_dividendInfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData.join(df_dividendInfo, on='Ticker') # by default: how='left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output DataFrames has all - and only - the indexes (and rows) of `df_refData`, with dividend data integrated from `df_dividendInfo`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation on this theme is a colum-on-column join. Is the situation in which the tickers are listed as a column `'Ticker'` in both DataFrames. Notice how we define `df_dividendInfo_TickerCol`, adding the ticker list as `'Ticker'` column (and leaving the index unspecified, thus using the default one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dividendInfo_TickerCol = pd.DataFrame(\n",
    "                    data={\n",
    "                        'Dividend Yield (%)': [0.72, 0.45, 1.15, 0.96, 2.01, 1.5, 0.3, 0.89],\n",
    "                        'Dividend Frequency': ['Quarterly', \n",
    "                                               'Monthly', \n",
    "                                               'Biannually',\n",
    "                                               'Quarterly',\n",
    "                                               'Biannually',\n",
    "                                               'Annually',\n",
    "                                               'Monthly',\n",
    "                                               'Quarterly'],\n",
    "                        'Ticker': ['ABC', 'BCD', 'CDE', 'DEF', 'EFG', 'FGH', 'GHI', 'HIJ']\n",
    "                        })\n",
    "\n",
    "df_dividendInfo_TickerCol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To (left- but also right-,inner- or outer-) join the two tables you can use `pd.merge()`, that we now introduce in its basic usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have two DataFrames, say `df1` and `df2`, you can combine them into a single DataFrame using function `pd.merge()` providing `df1` and `df2` in input. Basic syntax is \n",
    "\n",
    "```python\n",
    "pd.merge(df1, df2, how='inner', on=None)\n",
    "```\n",
    "\n",
    "which joins `df1` with `df2` on the base\n",
    "\n",
    "- of their indexes, if `on=None` (default), or\n",
    "- of a key column `col` if `on=col`. Column `col` needs to be found in both DataFrames.\n",
    "\n",
    "The `how` parameter keeps the same meaning as in `.join()` method with the only difference that the default behavior is the inner join `how='inner'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then left-join `df_refData` on `df_dividendInfo_TickerCol` on the `'Ticker'` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_refData, df_dividendInfo_TickerCol, how='left', on='Ticker')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior is that of a left-join, as we have already seen, on the `'Ticker'` column, shared by both DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the index (belonging to `df_refData`) is reset to the default one. This is a choice made from Pandas to homogeneize the behavior to more advanced merge operations, where you may end up with more rows if there are multiple matches. This is why Pandas does not keep the index for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to rename the index of `df_merged` in output, to the `df_refData` one, you can use `.rename()` method, which takes in input for its `index` parameter a dictionary of `current index: new index` pairs of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.rename(index = {current_index: new_index for current_index, new_index in zip(df_merged.index, df_refData.index)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we use `zip()` to loop over the two set of indexes of `df_merged` and `'df_refData`, taking `current_index` and `new_index` from `df_merged.index` and `df_refData.index`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last variation of the column-on-colum join is when the two columns, which are semantically equivalent (that is, they store the same kind of information), are named differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an example, let's rename `'Ticker'` column in `df_refData` as `'Id'`. We can use `.rename()` again, using its `columns` parameter giving in input a dictionary to rename `'Ticker'` column only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refData = df_refData.rename(columns = {'Ticker': 'Id'})\n",
    "df_refData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dividendInfo_TickerCol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left join of `df_refData` on `df_dividendInfo_TickerCol`, using `df_refData`'s `'Id'` column and `df_dividendInfo_TickerCol`'s `''Ticker'` column can be done using the `left_on` and `right_on` parameters of `pd.merge()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.merge(df1, df2, how='inner', on=None, left_on=None, right_on=None)\n",
    "```\n",
    "\n",
    "which, if `on=None` and `left_on='Col_in_df1'`, `right_on='Col_in_df2'` joins `df1` with `df2` on the base of `'Col_in_df1'` of `df1` and `'Col_in_df2'` of `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_refData, df_dividendInfo_TickerCol, how='left', left_on='Id', right_on='Ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a basic left-join as we have seen many, which you can reindex if you want. The only difference is the co-presence of `'Id'` and `'Ticker'` columns, with same matching values, of course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
